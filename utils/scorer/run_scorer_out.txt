Scoring a random baseline for task 1
INFO : Started evaluating results for Task 1 ...
INFO : Reading gold predictions from file /Users/pzg477/git/clef2019-factchecking-task1/data/training/20160926_1pres.tsv
INFO : Reading predicted ranking order from file /Users/pzg477/git/clef2019-factchecking-task1/scorer/data/task1_random_baseline.txt
INFO : ======================================== RESULTS for task1_random_baseline.txt =========================================
INFO : AVERAGE PRECISION:            0.0242    
INFO : ========================================================================================================================
INFO : RECIPROCAL RANK:              0.0172    
INFO : ========================================================================================================================
INFO : R-PRECISION (R=37):           0.0000    
INFO : ========================================================================================================================
INFO : PRECISION@N:                  @1        @3        @5        @10       @20       @50       
INFO :                               0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    
INFO : ========================================================================================================================
INFO : Description of the evaluation metrics: 
INFO : !!! THE OFFICIAL METRIC USED FOR THE COMPETITION RANKING IS MEAN AVERAGE PRECISION (MAP) !!!
INFO : R-Precision is Precision at R, where R is the number of relevant line_numbers for the evaluated set.
INFO : Average Precision is the precision@N, estimated only @ each relevant line_number and then averaged over the number of relevant line_numbers.
INFO : Reciprocal Rank is the reciprocal of the rank of the first relevant line_number in the list of predictions sorted by score (descendingly).
INFO : Precision@N is precision estimated for the first N line_numbers in the provided ranked list.
INFO : The MEAN versions of each metric are provided to average over multiple debates (each with separate prediction file).
INFO : ========================================================================================================================
INFO : ========================================================================================================================
**********
Scoring the gold predictions for task 1
INFO : Started evaluating results for Task 1 ...
INFO : Reading gold predictions from file /Users/pzg477/git/clef2019-factchecking-task1/data/training/20160926_1pres.tsv
INFO : Reading predicted ranking order from file /Users/pzg477/git/clef2019-factchecking-task1/scorer/data/task1_gold.txt
INFO : ============================================== RESULTS for task1_gold.txt ==============================================
INFO : AVERAGE PRECISION:            1.0000    
INFO : ========================================================================================================================
INFO : RECIPROCAL RANK:              1.0000    
INFO : ========================================================================================================================
INFO : R-PRECISION (R=37):           1.0000    
INFO : ========================================================================================================================
INFO : PRECISION@N:                  @1        @3        @5        @10       @20       @50       
INFO :                               1.0000    1.0000    1.0000    1.0000    1.0000    0.7400    
INFO : ========================================================================================================================
INFO : Description of the evaluation metrics: 
INFO : !!! THE OFFICIAL METRIC USED FOR THE COMPETITION RANKING IS MEAN AVERAGE PRECISION (MAP) !!!
INFO : R-Precision is Precision at R, where R is the number of relevant line_numbers for the evaluated set.
INFO : Average Precision is the precision@N, estimated only @ each relevant line_number and then averaged over the number of relevant line_numbers.
INFO : Reciprocal Rank is the reciprocal of the rank of the first relevant line_number in the list of predictions sorted by score (descendingly).
INFO : Precision@N is precision estimated for the first N line_numbers in the provided ranked list.
INFO : The MEAN versions of each metric are provided to average over multiple debates (each with separate prediction file).
INFO : ========================================================================================================================
INFO : ========================================================================================================================
**********
Scoring BOTH the gold and random baseline for task 1 (only for example purposes, scoring multiple files for 1 debate will lead to wrong metrics)
INFO : Started evaluating results for Task 1 ...
INFO : Reading gold predictions from file /Users/pzg477/git/clef2019-factchecking-task1/data/training/20160926_1pres.tsv
INFO : Reading predicted ranking order from file /Users/pzg477/git/clef2019-factchecking-task1/scorer/data/task1_gold.txt
INFO : ============================================== RESULTS for task1_gold.txt ==============================================
INFO : AVERAGE PRECISION:            1.0000    
INFO : ========================================================================================================================
INFO : RECIPROCAL RANK:              1.0000    
INFO : ========================================================================================================================
INFO : R-PRECISION (R=37):           1.0000    
INFO : ========================================================================================================================
INFO : PRECISION@N:                  @1        @3        @5        @10       @20       @50       
INFO :                               1.0000    1.0000    1.0000    1.0000    1.0000    0.7400    
INFO : ========================================================================================================================
INFO : Reading gold predictions from file /Users/pzg477/git/clef2019-factchecking-task1/data/training/20160926_1pres.tsv
INFO : Reading predicted ranking order from file /Users/pzg477/git/clef2019-factchecking-task1/scorer/data/task1_random_baseline.txt
INFO : ======================================== RESULTS for task1_random_baseline.txt =========================================
INFO : AVERAGE PRECISION:            0.0242    
INFO : ========================================================================================================================
INFO : RECIPROCAL RANK:              0.0172    
INFO : ========================================================================================================================
INFO : R-PRECISION (R=37):           0.0000    
INFO : ========================================================================================================================
INFO : PRECISION@N:                  @1        @3        @5        @10       @20       @50       
INFO :                               0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    
INFO : ========================================================================================================================
INFO : =================================================== AVERAGED RESULTS ===================================================
INFO : MEAN AVERAGE PRECISION (MAP): 0.5121    
INFO : ========================================================================================================================
INFO : MEAN RECIPROCAL RANK:         0.5086    
INFO : ========================================================================================================================
INFO : MEAN R-PRECISION:             0.5000    
INFO : ========================================================================================================================
INFO : MEAN PRECISION@N:             @1        @3        @5        @10       @20       @50       
INFO :                               0.5000    0.5000    0.5000    0.5000    0.5000    0.3700    
INFO : ========================================================================================================================
INFO : Description of the evaluation metrics: 
INFO : !!! THE OFFICIAL METRIC USED FOR THE COMPETITION RANKING IS MEAN AVERAGE PRECISION (MAP) !!!
INFO : R-Precision is Precision at R, where R is the number of relevant line_numbers for the evaluated set.
INFO : Average Precision is the precision@N, estimated only @ each relevant line_number and then averaged over the number of relevant line_numbers.
INFO : Reciprocal Rank is the reciprocal of the rank of the first relevant line_number in the list of predictions sorted by score (descendingly).
INFO : Precision@N is precision estimated for the first N line_numbers in the provided ranked list.
INFO : The MEAN versions of each metric are provided to average over multiple debates (each with separate prediction file).
INFO : ========================================================================================================================
INFO : ========================================================================================================================
